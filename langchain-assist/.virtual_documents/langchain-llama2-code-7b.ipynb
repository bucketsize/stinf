


from langchain_community.llms import Ollama
from langchain_core.output_parsers import StrOutputParser
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.prompts import PromptTemplate


llm = Ollama(model="codellama")


p_sys = """you are a top coder who can write high performance and functionally correct code"""
t_sys = """<s>[INST] <<SYS>>
{p_sys}
<</SYS>>

{p_user} [/INST]"""

prompt = PromptTemplate(
    template=t_sys,
    input_variables=["p_sys", "p_user"]
)



chain = prompt | llm | StrOutputParser()


q1 = "write a python script to copy linux root filesystem to a virtualbox image"
async for chunk in chain.astream({"p_sys": p_sys, "p_user": q1}):
    print(chunk, end="", flush=True) 
